\section{Discussion}

One of the significant accomplishments in this semester is the successful integration of the QR code tracking script and drone movement script on the Jetson Nano 2GB. The QR code tracking script allowed the drone to detect QR codes and rotate to track them. The drone could also turn right until the QR code was in the center, showing the potential for autonomous navigation using computer vision techniques.

The results from the first MiDaS evaluation experiment reveal that it is reasonably accurate to perform object detection and depth estimation using YOLOv5 and MiDaS models in an indoor environment. However, the outdoor experiment simulating a practical object depth estimation task yields unfavorable results. The depth map intensity is not as accurate as expected and is significantly affected by various factors, such as sunlight orientation, webcam exposure, and drone vibration. Although the MiDaS model is reliable in a stable environment with minimal interference, its application in autonomous drone pollination tasks remains challenging.

These results demonstrate the progress made in the project and pave the way for future developments. In the next semester, we will further research solutions for autonomous drone navigation from the current position to the target position. We will consider either developing a new algorithm utilizing a stereo camera with higher accuracy or exploring alternative methods, such as target and self-localization to determine the relative position first, followed by motion planning. This approach will hopefully enable us to improve the reliability of the autonomous drone navigation system in real-world scenarios.